{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e923fbf",
   "metadata": {},
   "source": [
    "# AI Job Market - Data Cleaning & Enrichment Documentation\n",
    "\n",
    "**Purpose**: This notebook documents the complete data cleaning and enrichment pipeline for the AI Job Market dataset.\n",
    "\n",
    "**Dataset Source**: \n",
    "- Kaggle: AI Job Market Dataset\n",
    "- Local: `data/raw/ai_job_market.csv`\n",
    "\n",
    "**Workflow**:\n",
    "1. Data Loading (Kaggle + Local)\n",
    "2. Data Exploration\n",
    "3. Data Cleaning (duplicates, missing values, validation)\n",
    "4. Data Enrichment (salary, skills, tools, location, experience, date)\n",
    "5. Save Cleaned & Enriched Data\n",
    "\n",
    "**Output**:\n",
    "- Cleaned dataset: `data/cleaned/ai_job_market_cleaned.csv`\n",
    "- Enriched datasets: `data/enriched/` (by category)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259c866",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import all necessary libraries and configure the environment for data loading, cleaning, and enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51721815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "✓ Project root: c:\\Users\\Admin\\project\\Data Analysis\\ai_job_market\n",
      "✓ Current time: 2025-12-19 12:53:13\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Project-specific imports\n",
    "from utils.config_loader import get_config_loader\n",
    "from utils.file_handler import FileHandler\n",
    "from utils.data_cleaner import DataCleaner\n",
    "from utils.data_validator import DataValidator\n",
    "from utils.logger import get_logger\n",
    "from utils.enrichers import (\n",
    "    SalaryEnricher, SkillsEnricher, ToolsEnricher,\n",
    "    ExperienceEnricher, LocationEnricher, DateEnricher,\n",
    "    AdditionalFeaturesEnricher\n",
    ")\n",
    "\n",
    "# Initialize\n",
    "config = get_config_loader()\n",
    "file_handler = FileHandler()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "print(f\"✓ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aec4a6",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load data from multiple sources:\n",
    "- **Kaggle API**: Download directly from Kaggle (if credentials are configured)\n",
    "- **Local Storage**: Load from `data/raw/ai_job_market.csv`\n",
    "\n",
    "### 2.1 Configure Kaggle API (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efba4da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_kaggle(dataset_name: str, download_path: str = 'data/raw') -> bool:\n",
    "    try:\n",
    "        import kaggle\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        \n",
    "        # Authenticate\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        # Download dataset\n",
    "        print(f\"Downloading dataset: {dataset_name}\")\n",
    "        api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "        print(f\"✓ Dataset downloaded to: {download_path}\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠ Kaggle API not installed. Install with: pip install kaggle\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error downloading from Kaggle: {str(e)}\")\n",
    "        print(\"Falling back to local data...\")\n",
    "        return False\n",
    "\n",
    "# Uncomment and modify to download from Kaggle\n",
    "# KAGGLE_DATASET = 'your-username/ai-job-market'\n",
    "# load_from_kaggle(KAGGLE_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0e0a6",
   "metadata": {},
   "source": [
    "### 2.2 Load from Local Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54647e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 12:59:03,997 - utils.file_handler - INFO - Loaded CSV: c:\\Users\\Admin\\project\\Data Analysis\\ai_job_market\\data\\raw\\ai_job_market.csv with shape (2000, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: data/raw/ai_job_market.csv\n",
      "Encoding: utf-8\n",
      "Delimiter: ,\n",
      "------------------------------------------------------------\n",
      "\n",
      "✓ Data loaded successfully!\n",
      "  Shape: 2,000 rows × 12 columns\n",
      "  Columns: ['job_id', 'company_name', 'industry', 'job_title', 'skills_required', 'experience_level', 'employment_type', 'location', 'salary_range_usd', 'posted_date', 'company_size', 'tools_preferred']\n",
      "  Memory usage: 1.33 MB\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "# Fix config path for notebook context\n",
    "config.config_dir = project_root / 'config'\n",
    "\n",
    "paths_config = config.load('paths')\n",
    "raw_data_path = paths_config['paths']['raw_data_file']\n",
    "loading_config = paths_config['data_processing']['loading']\n",
    "\n",
    "print(f\"Loading data from: {raw_data_path}\")\n",
    "print(f\"Encoding: {loading_config['encoding']}\")\n",
    "print(f\"Delimiter: {loading_config['delimiter']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load raw data with absolute path\n",
    "raw_data_path_abs = project_root / raw_data_path\n",
    "df_raw = file_handler.load_csv(\n",
    "    str(raw_data_path_abs),\n",
    "    encoding=loading_config['encoding'],\n",
    "    delimiter=loading_config['delimiter']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"  Shape: {df_raw.shape[0]:,} rows × {df_raw.shape[1]} columns\")\n",
    "print(f\"  Columns: {list(df_raw.columns)}\")\n",
    "print(f\"  Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce4cfc",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Initial exploration to understand the dataset structure, quality, and issues before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcc195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>job_title</th>\n",
       "      <th>skills_required</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_range_usd</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>company_size</th>\n",
       "      <th>tools_preferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Foster and Sons</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>NumPy, Reinforcement Learning, PyTorch, Scikit...</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Tracybury, AR</td>\n",
       "      <td>92860-109598</td>\n",
       "      <td>2025-08-20</td>\n",
       "      <td>Large</td>\n",
       "      <td>KDB+, LangChain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Boyd, Myers and Ramirez</td>\n",
       "      <td>Tech</td>\n",
       "      <td>Computer Vision Engineer</td>\n",
       "      <td>Scikit-learn, CUDA, SQL, Pandas</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Lake Scott, CU</td>\n",
       "      <td>78523-144875</td>\n",
       "      <td>2024-03-22</td>\n",
       "      <td>Large</td>\n",
       "      <td>FastAPI, KDB+, TensorFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>King Inc</td>\n",
       "      <td>Tech</td>\n",
       "      <td>Quant Researcher</td>\n",
       "      <td>MLflow, FastAPI, Azure, PyTorch, SQL, GCP</td>\n",
       "      <td>Entry</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>East Paige, CM</td>\n",
       "      <td>124496-217204</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>Large</td>\n",
       "      <td>BigQuery, PyTorch, Scikit-learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Cooper, Archer and Lynch</td>\n",
       "      <td>Tech</td>\n",
       "      <td>AI Product Manager</td>\n",
       "      <td>Scikit-learn, C++, Pandas, LangChain, AWS, R</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Perezview, FI</td>\n",
       "      <td>50908-123743</td>\n",
       "      <td>2024-05-08</td>\n",
       "      <td>Large</td>\n",
       "      <td>TensorFlow, BigQuery, MLflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hall LLC</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Excel, Keras, SQL, Hugging Face</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Contract</td>\n",
       "      <td>North Desireeland, NE</td>\n",
       "      <td>98694-135413</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>Large</td>\n",
       "      <td>PyTorch, LangChain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id              company_name    industry                 job_title  \\\n",
       "0       1           Foster and Sons  Healthcare              Data Analyst   \n",
       "1       2   Boyd, Myers and Ramirez        Tech  Computer Vision Engineer   \n",
       "2       3                  King Inc        Tech          Quant Researcher   \n",
       "3       4  Cooper, Archer and Lynch        Tech        AI Product Manager   \n",
       "4       5                  Hall LLC     Finance            Data Scientist   \n",
       "\n",
       "                                     skills_required experience_level  \\\n",
       "0  NumPy, Reinforcement Learning, PyTorch, Scikit...              Mid   \n",
       "1                    Scikit-learn, CUDA, SQL, Pandas           Senior   \n",
       "2          MLflow, FastAPI, Azure, PyTorch, SQL, GCP            Entry   \n",
       "3       Scikit-learn, C++, Pandas, LangChain, AWS, R              Mid   \n",
       "4                    Excel, Keras, SQL, Hugging Face           Senior   \n",
       "\n",
       "  employment_type               location salary_range_usd posted_date  \\\n",
       "0       Full-time          Tracybury, AR     92860-109598  2025-08-20   \n",
       "1       Full-time         Lake Scott, CU     78523-144875  2024-03-22   \n",
       "2       Full-time         East Paige, CM    124496-217204  2025-09-18   \n",
       "3       Full-time          Perezview, FI     50908-123743  2024-05-08   \n",
       "4        Contract  North Desireeland, NE     98694-135413  2025-02-24   \n",
       "\n",
       "  company_size                  tools_preferred  \n",
       "0        Large                  KDB+, LangChain  \n",
       "1        Large        FastAPI, KDB+, TensorFlow  \n",
       "2        Large  BigQuery, PyTorch, Scikit-learn  \n",
       "3        Large     TensorFlow, BigQuery, MLflow  \n",
       "4        Large               PyTorch, LangChain  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>job_title</th>\n",
       "      <th>skills_required</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_range_usd</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>company_size</th>\n",
       "      <th>tools_preferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1996</td>\n",
       "      <td>Mueller, Ellis and Clark</td>\n",
       "      <td>Finance</td>\n",
       "      <td>NLP Engineer</td>\n",
       "      <td>Flask, FastAPI, Power BI</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Internship</td>\n",
       "      <td>Washingtonmouth, SD</td>\n",
       "      <td>90382-110126</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>Large</td>\n",
       "      <td>MLflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1997</td>\n",
       "      <td>Roberts-Yu</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>AI Product Manager</td>\n",
       "      <td>R, Flask, Excel, C++, CUDA, Scikit-learn</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Joshuafort, ZA</td>\n",
       "      <td>47848-137195</td>\n",
       "      <td>2023-12-02</td>\n",
       "      <td>Large</td>\n",
       "      <td>KDB+, LangChain, MLflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1998</td>\n",
       "      <td>Brooks, Williams and Randolph</td>\n",
       "      <td>Education</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Hugging Face, Excel, Scikit-learn, R, MLflow</td>\n",
       "      <td>Entry</td>\n",
       "      <td>Contract</td>\n",
       "      <td>West Brittanyburgh, CG</td>\n",
       "      <td>134994-180108</td>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>Large</td>\n",
       "      <td>PyTorch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1999</td>\n",
       "      <td>Castaneda-Smith</td>\n",
       "      <td>Education</td>\n",
       "      <td>Quant Researcher</td>\n",
       "      <td>AWS, Python, Scikit-learn</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Contract</td>\n",
       "      <td>Anthonyshire, OM</td>\n",
       "      <td>62388-82539</td>\n",
       "      <td>2024-08-10</td>\n",
       "      <td>Large</td>\n",
       "      <td>MLflow, TensorFlow, FastAPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2000</td>\n",
       "      <td>Estes Group</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Quant Researcher</td>\n",
       "      <td>Flask, TensorFlow, Power BI</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Benjaminview, NE</td>\n",
       "      <td>55835-97374</td>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>Startup</td>\n",
       "      <td>MLflow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      job_id                   company_name    industry           job_title  \\\n",
       "1995    1996       Mueller, Ellis and Clark     Finance        NLP Engineer   \n",
       "1996    1997                     Roberts-Yu  Automotive  AI Product Manager   \n",
       "1997    1998  Brooks, Williams and Randolph   Education        Data Analyst   \n",
       "1998    1999                Castaneda-Smith   Education    Quant Researcher   \n",
       "1999    2000                    Estes Group     Finance    Quant Researcher   \n",
       "\n",
       "                                   skills_required experience_level  \\\n",
       "1995                      Flask, FastAPI, Power BI           Senior   \n",
       "1996      R, Flask, Excel, C++, CUDA, Scikit-learn              Mid   \n",
       "1997  Hugging Face, Excel, Scikit-learn, R, MLflow            Entry   \n",
       "1998                     AWS, Python, Scikit-learn           Senior   \n",
       "1999                   Flask, TensorFlow, Power BI           Senior   \n",
       "\n",
       "     employment_type                location salary_range_usd posted_date  \\\n",
       "1995      Internship     Washingtonmouth, SD     90382-110126  2024-04-22   \n",
       "1996          Remote          Joshuafort, ZA     47848-137195  2023-12-02   \n",
       "1997        Contract  West Brittanyburgh, CG    134994-180108  2023-10-29   \n",
       "1998        Contract        Anthonyshire, OM      62388-82539  2024-08-10   \n",
       "1999       Full-time        Benjaminview, NE      55835-97374  2025-02-20   \n",
       "\n",
       "     company_size              tools_preferred  \n",
       "1995        Large                       MLflow  \n",
       "1996        Large      KDB+, LangChain, MLflow  \n",
       "1997        Large                      PyTorch  \n",
       "1998        Large  MLflow, TensorFlow, FastAPI  \n",
       "1999      Startup                       MLflow  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET INFORMATION\n",
      "============================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   job_id            2000 non-null   int64 \n",
      " 1   company_name      2000 non-null   object\n",
      " 2   industry          2000 non-null   object\n",
      " 3   job_title         2000 non-null   object\n",
      " 4   skills_required   2000 non-null   object\n",
      " 5   experience_level  2000 non-null   object\n",
      " 6   employment_type   2000 non-null   object\n",
      " 7   location          2000 non-null   object\n",
      " 8   salary_range_usd  2000 non-null   object\n",
      " 9   posted_date       2000 non-null   object\n",
      " 10  company_size      2000 non-null   object\n",
      " 11  tools_preferred   2000 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 187.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display first and last rows\n",
    "print(\"First 5 rows:\")\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "display(df_raw.tail())\n",
    "\n",
    "# Dataset info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aff527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "1. MISSING VALUES: 0 total\n",
      "\n",
      "2. DUPLICATES: 0 rows\n",
      "\n",
      "3. SUMMARY:\n",
      "   Total Rows: 2,000\n",
      "   Total Columns: 12\n",
      "   Missing Values: 0\n",
      "   Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Data quality check using DataValidator\n",
    "validator = DataValidator(df_raw)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check missing values\n",
    "missing_info = validator.check_missing_values()\n",
    "print(f\"\\n1. MISSING VALUES: {missing_info['total_missing']} total\")\n",
    "if missing_info['total_missing'] > 0:\n",
    "    missing_df = pd.DataFrame(missing_info['by_column']).T\n",
    "    missing_df = missing_df[missing_df['count'] > 0].sort_values('count', ascending=False)\n",
    "    display(missing_df)\n",
    "\n",
    "# Check duplicates\n",
    "duplicate_info = validator.check_duplicates()\n",
    "print(f\"\\n2. DUPLICATES: {duplicate_info['count']} rows\")\n",
    "if duplicate_info['count'] > 0:\n",
    "    print(f\"   Percentage: {duplicate_info['percentage']:.2f}%\")\n",
    "\n",
    "# Get validation summary\n",
    "summary = validator.get_summary()\n",
    "print(f\"\\n3. SUMMARY:\")\n",
    "for key, value in summary.items():\n",
    "    if key == 'shape':\n",
    "        print(f\"   Total Rows: {value[0]:,}\")\n",
    "        print(f\"   Total Columns: {value[1]}\")\n",
    "    elif key == 'validation_results':\n",
    "        print(f\"   Missing Values: {value['missing_values']['total_missing']}\")\n",
    "        print(f\"   Duplicates: {value['duplicates']['count']}\")\n",
    "    elif key == 'data_quality_score':\n",
    "        print(f\"   Data Quality Score: {value:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b3181",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "Clean the dataset by:\n",
    "1. **Removing duplicates**: Eliminate duplicate job postings\n",
    "2. **Handling missing values**: Drop rows with missing critical data\n",
    "3. **Validating results**: Ensure cleaning improved data quality\n",
    "\n",
    "The `DataCleaner` class uses a fluent interface for chainable operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109f387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:03:56,941 - utils.data_cleaner - INFO - Removed 0 duplicate rows\n",
      "2025-12-19 13:03:56,977 - utils.data_cleaner - INFO - Handled missing values with strategy 'drop': 0 -> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA CLEANING PROCESS\n",
      "============================================================\n",
      "\n",
      "Original dataset shape: (2000, 12)\n",
      "✓ Step 1: Removed duplicates\n",
      "✓ Step 2: Handled missing values (strategy: drop)\n",
      "\n",
      "Cleaned dataset shape: (2000, 12)\n",
      "Total rows removed: 0\n",
      "Rows retained: 100.00%\n",
      "\n",
      "============================================================\n",
      "CLEANING OPERATIONS SUMMARY\n",
      "============================================================\n",
      "\n",
      "1. REMOVE_DUPLICATES\n",
      "   rows_removed: 0\n",
      "   subset: None\n",
      "   keep: first\n",
      "\n",
      "2. HANDLE_MISSING_VALUES\n",
      "   strategy: drop\n",
      "   missing_before: 0\n",
      "   missing_after: 0\n",
      "   rows_removed: 0\n",
      "   columns: ['job_id', 'company_name', 'industry', 'job_title', 'skills_required', 'experience_level', 'employment_type', 'location', 'salary_range_usd', 'posted_date', 'company_size', 'tools_preferred']\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataCleaner with the raw dataset\n",
    "cleaner = DataCleaner(df_raw)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING PROCESS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOriginal dataset shape: {df_raw.shape}\")\n",
    "\n",
    "# Step 1: Remove duplicates\n",
    "cleaning_config = paths_config['data_processing']['cleaning']\n",
    "if cleaning_config['remove_duplicates']:\n",
    "    cleaner.remove_duplicates()\n",
    "    print(\"✓ Step 1: Removed duplicates\")\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "if cleaning_config['handle_missing_values']:\n",
    "    strategy = cleaning_config['missing_strategy']\n",
    "    fill_value = cleaning_config.get('fill_value')\n",
    "    cleaner.handle_missing_values(strategy=strategy, fill_value=fill_value)\n",
    "    print(f\"✓ Step 2: Handled missing values (strategy: {strategy})\")\n",
    "\n",
    "# Get cleaned data and report\n",
    "df_cleaned = cleaner.get_cleaned_data()\n",
    "cleaning_report = cleaner.get_report()\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Total rows removed: {cleaning_report['total_rows_removed']}\")\n",
    "print(f\"Rows retained: {df_cleaned.shape[0] / df_raw.shape[0] * 100:.2f}%\")\n",
    "\n",
    "# Display cleaning operations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANING OPERATIONS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for i, operation in enumerate(cleaning_report['operations'], 1):\n",
    "    print(f\"\\n{i}. {operation['operation'].upper()}\")\n",
    "    for key, value in operation.items():\n",
    "        if key != 'operation':\n",
    "            print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbda664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "POST-CLEANING VALIDATION\n",
      "============================================================\n",
      "\n",
      "✓ Missing values: 0\n",
      "✓ Duplicates: 0\n",
      "\n",
      "First 3 rows of cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>industry</th>\n",
       "      <th>job_title</th>\n",
       "      <th>skills_required</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_range_usd</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>company_size</th>\n",
       "      <th>tools_preferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Foster and Sons</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>NumPy, Reinforcement Learning, PyTorch, Scikit...</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Tracybury, AR</td>\n",
       "      <td>92860-109598</td>\n",
       "      <td>2025-08-20</td>\n",
       "      <td>Large</td>\n",
       "      <td>KDB+, LangChain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Boyd, Myers and Ramirez</td>\n",
       "      <td>Tech</td>\n",
       "      <td>Computer Vision Engineer</td>\n",
       "      <td>Scikit-learn, CUDA, SQL, Pandas</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Lake Scott, CU</td>\n",
       "      <td>78523-144875</td>\n",
       "      <td>2024-03-22</td>\n",
       "      <td>Large</td>\n",
       "      <td>FastAPI, KDB+, TensorFlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>King Inc</td>\n",
       "      <td>Tech</td>\n",
       "      <td>Quant Researcher</td>\n",
       "      <td>MLflow, FastAPI, Azure, PyTorch, SQL, GCP</td>\n",
       "      <td>Entry</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>East Paige, CM</td>\n",
       "      <td>124496-217204</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>Large</td>\n",
       "      <td>BigQuery, PyTorch, Scikit-learn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id             company_name    industry                 job_title  \\\n",
       "0       1          Foster and Sons  Healthcare              Data Analyst   \n",
       "1       2  Boyd, Myers and Ramirez        Tech  Computer Vision Engineer   \n",
       "2       3                 King Inc        Tech          Quant Researcher   \n",
       "\n",
       "                                     skills_required experience_level  \\\n",
       "0  NumPy, Reinforcement Learning, PyTorch, Scikit...              Mid   \n",
       "1                    Scikit-learn, CUDA, SQL, Pandas           Senior   \n",
       "2          MLflow, FastAPI, Azure, PyTorch, SQL, GCP            Entry   \n",
       "\n",
       "  employment_type        location salary_range_usd posted_date company_size  \\\n",
       "0       Full-time   Tracybury, AR     92860-109598  2025-08-20        Large   \n",
       "1       Full-time  Lake Scott, CU     78523-144875  2024-03-22        Large   \n",
       "2       Full-time  East Paige, CM    124496-217204  2025-09-18        Large   \n",
       "\n",
       "                   tools_preferred  \n",
       "0                  KDB+, LangChain  \n",
       "1        FastAPI, KDB+, TensorFlow  \n",
       "2  BigQuery, PyTorch, Scikit-learn  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate cleaned data\n",
    "validator_cleaned = DataValidator(df_cleaned)\n",
    "missing_after = validator_cleaned.check_missing_values()\n",
    "duplicates_after = validator_cleaned.check_duplicates()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"POST-CLEANING VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✓ Missing values: {missing_after['total_missing']}\")\n",
    "print(f\"✓ Duplicates: {duplicates_after['count']}\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\nFirst 3 rows of cleaned data:\")\n",
    "display(df_cleaned.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674320c3",
   "metadata": {},
   "source": [
    "## 5. Data Enrichment\n",
    "\n",
    "Enrich the cleaned dataset with additional features across multiple categories:\n",
    "\n",
    "### Enrichment Categories:\n",
    "1. **Salary**: Parse min/max/avg, create salary clusters\n",
    "2. **Skills**: Extract top 20 skills, count skills, create binary features\n",
    "3. **Tools**: Extract top 15 tools, count tools, create binary features\n",
    "4. **Experience**: Convert to ordinal encoding (Entry=1, Mid=2, Senior=3)\n",
    "5. **Location**: Parse city/state, cluster locations, USA vs International\n",
    "6. **Date**: Extract year/month/quarter, calculate job age\n",
    "7. **Additional**: Employment type flags, company features\n",
    "\n",
    "Each enricher is a separate class for modularity and reusability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429ab22",
   "metadata": {},
   "source": [
    "### 5.1 Salary Enrichment\n",
    "\n",
    "Parse salary ranges and create salary clusters for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39d4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:04:36,349 - utils.enrichers - INFO - Parsed salary ranges into min, max, and avg\n",
      "2025-12-19 13:04:36,398 - utils.enrichers - INFO - Created salary clusters with 7 categories\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENRICHMENT 1: SALARY FEATURES\n",
      "============================================================\n",
      "\n",
      "✓ Parsed salary ranges:\n",
      "  - salary_min: minimum salary\n",
      "  - salary_max: maximum salary\n",
      "  - salary_avg: average salary\n",
      "\n",
      "✓ Created salary clusters:\n",
      "salary_cluster\n",
      "<60K         58\n",
      "60-80K      199\n",
      "80-100K     322\n",
      "100-120K    360\n",
      "120-150K    540\n",
      "150-200K    521\n",
      "200K+         0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample salary data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary_range_usd</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>salary_avg</th>\n",
       "      <th>salary_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92860-109598</td>\n",
       "      <td>92860</td>\n",
       "      <td>109598</td>\n",
       "      <td>101229.0</td>\n",
       "      <td>100-120K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78523-144875</td>\n",
       "      <td>78523</td>\n",
       "      <td>144875</td>\n",
       "      <td>111699.0</td>\n",
       "      <td>100-120K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124496-217204</td>\n",
       "      <td>124496</td>\n",
       "      <td>217204</td>\n",
       "      <td>170850.0</td>\n",
       "      <td>150-200K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  salary_range_usd  salary_min  salary_max  salary_avg salary_cluster\n",
       "0     92860-109598       92860      109598    101229.0       100-120K\n",
       "1     78523-144875       78523      144875    111699.0       100-120K\n",
       "2    124496-217204      124496      217204    170850.0       150-200K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENRICHMENT 1: SALARY FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy for enrichment\n",
    "df_enriched = df_cleaned.copy()\n",
    "\n",
    "# Enrich salary data\n",
    "salary_enricher = SalaryEnricher(df_enriched)\n",
    "df_enriched = salary_enricher.enrich()\n",
    "\n",
    "print(\"\\n✓ Parsed salary ranges:\")\n",
    "print(f\"  - salary_min: minimum salary\")\n",
    "print(f\"  - salary_max: maximum salary\")\n",
    "print(f\"  - salary_avg: average salary\")\n",
    "\n",
    "print(\"\\n✓ Created salary clusters:\")\n",
    "print(df_enriched['salary_cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSample salary data:\")\n",
    "display(df_enriched[['salary_range_usd', 'salary_min', 'salary_max', 'salary_avg', 'salary_cluster']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2e623",
   "metadata": {},
   "source": [
    "### 5.2 Skills Enrichment\n",
    "\n",
    "Extract and analyze skills from job postings. Creates binary features for top 20 skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c20325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:04:48,798 - utils.enrichers - INFO - Parsed 20 top skills as binary features\n",
      "2025-12-19 13:04:48,819 - utils.enrichers - INFO - Created skill category flags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENRICHMENT 2: SKILLS FEATURES\n",
      "============================================================\n",
      "\n",
      "✓ Created binary features for top 20 skills\n",
      "✓ Created skill category flags:\n",
      "  - has_programming_lang: 1202 jobs\n",
      "  - has_cloud_platform: 1005 jobs\n",
      "  - has_ml_framework: 1247 jobs\n",
      "\n",
      "Top 10 most demanded skills:\n",
      "TensorFlow                452\n",
      "Excel                     432\n",
      "Pandas                    427\n",
      "FastAPI                   419\n",
      "NumPy                     416\n",
      "Reinforcement Learning    414\n",
      "Azure                     413\n",
      "SQL                       408\n",
      "Hugging Face              408\n",
      "Keras                     406\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Created 20 skill binary features\n",
      "Sample: ['skill_tensorflow', 'skill_excel', 'skill_pandas', 'skill_fastapi', 'skill_numpy']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENRICHMENT 2: SKILLS FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enrich skills data\n",
    "skills_enricher = SkillsEnricher(df_enriched, top_n=20)\n",
    "df_enriched, skill_counts = skills_enricher.enrich()\n",
    "\n",
    "print(\"\\n✓ Created binary features for top 20 skills\")\n",
    "print(\"✓ Created skill category flags:\")\n",
    "print(f\"  - has_programming_lang: {df_enriched['has_programming_lang'].sum()} jobs\")\n",
    "print(f\"  - has_cloud_platform: {df_enriched['has_cloud_platform'].sum()} jobs\")\n",
    "print(f\"  - has_ml_framework: {df_enriched['has_ml_framework'].sum()} jobs\")\n",
    "\n",
    "print(\"\\nTop 10 most demanded skills:\")\n",
    "print(skill_counts.head(10))\n",
    "\n",
    "# Show skill columns\n",
    "skill_columns = [col for col in df_enriched.columns if col.startswith('skill_')]\n",
    "print(f\"\\n✓ Created {len(skill_columns)} skill binary features\")\n",
    "print(f\"Sample: {skill_columns[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a43cb",
   "metadata": {},
   "source": [
    "### 5.3 Tools Enrichment\n",
    "\n",
    "Extract and analyze preferred tools from job postings. Creates binary features for top 15 tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ef0dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:05:12,701 - utils.enrichers - INFO - Parsed 8 top tools as binary features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENRICHMENT 3: TOOLS FEATURES\n",
      "============================================================\n",
      "\n",
      "✓ Created binary features for top 15 tools\n",
      "✓ Created tools_count feature\n",
      "\n",
      "Top 10 most preferred tools:\n",
      "MLflow          513\n",
      "LangChain       511\n",
      "FastAPI         505\n",
      "KDB+            499\n",
      "BigQuery        494\n",
      "TensorFlow      487\n",
      "PyTorch         475\n",
      "Scikit-learn    474\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Created 8 tool binary features\n",
      "Sample: ['tool_mlflow', 'tool_langchain', 'tool_fastapi', 'tool_kdbplus', 'tool_bigquery']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENRICHMENT 3: TOOLS FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enrich tools data\n",
    "tools_enricher = ToolsEnricher(df_enriched, top_n=15)\n",
    "df_enriched, tool_counts = tools_enricher.enrich()\n",
    "\n",
    "print(\"\\n✓ Created binary features for top 15 tools\")\n",
    "print(f\"✓ Created tools_count feature\")\n",
    "\n",
    "print(\"\\nTop 10 most preferred tools:\")\n",
    "print(tool_counts.head(10))\n",
    "\n",
    "# Show tool columns\n",
    "tool_columns = [col for col in df_enriched.columns if col.startswith('tool_')]\n",
    "print(f\"\\n✓ Created {len(tool_columns)} tool binary features\")\n",
    "print(f\"Sample: {tool_columns[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d34fcf",
   "metadata": {},
   "source": [
    "### 5.4 Experience Level Enrichment\n",
    "\n",
    "Convert experience levels to ordinal encoding for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1298788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:05:19,903 - utils.enrichers - INFO - Created ordinal encoding for experience levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENRICHMENT 4: EXPERIENCE LEVEL\n",
      "============================================================\n",
      "\n",
      "✓ Converted experience level to ordinal encoding:\n",
      "  Entry Level = 1\n",
      "  Mid Level = 2\n",
      "  Senior Level = 3\n",
      "\n",
      "Experience level distribution:\n",
      "experience_level_ordinal\n",
      "1    702\n",
      "2    668\n",
      "3    630\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENRICHMENT 4: EXPERIENCE LEVEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enrich experience data\n",
    "experience_enricher = ExperienceEnricher(df_enriched)\n",
    "df_enriched = experience_enricher.enrich()\n",
    "\n",
    "print(\"\\n✓ Converted experience level to ordinal encoding:\")\n",
    "print(\"  Entry Level = 1\")\n",
    "print(\"  Mid Level = 2\")\n",
    "print(\"  Senior Level = 3\")\n",
    "\n",
    "print(\"\\nExperience level distribution:\")\n",
    "print(df_enriched['experience_level_ordinal'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f26f7",
   "metadata": {},
   "source": [
    "### 5.5 Location Enrichment\n",
    "\n",
    "Parse location data and create geographic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19cdde3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENRICHMENT 5: LOCATION FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:06:55,134 - utils.enrichers - INFO - Parsed location into city, state, cluster, and region\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Parsed location into:\n",
      "  - location_city\n",
      "  - location_state\n",
      "  - location_region (e.g., Northeast, West)\n",
      "  - is_usa (1 for USA, 0 for International)\n",
      "\n",
      "Top 10 states by job count:\n",
      "location_state\n",
      "PG    19\n",
      "BB    18\n",
      "FJ    18\n",
      "HR    18\n",
      "BT    18\n",
      "IQ    17\n",
      "JO    17\n",
      "UZ    16\n",
      "JM    16\n",
      "GQ    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ USA jobs: 227\n",
      "✓ International jobs: 1773\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENRICHMENT 5: LOCATION FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enrich location data\n",
    "location_enricher = LocationEnricher(df_enriched)\n",
    "df_enriched, state_counts = location_enricher.enrich()\n",
    "\n",
    "print(\"\\n✓ Parsed location into:\")\n",
    "print(\"  - location_city\")\n",
    "print(\"  - location_state\")\n",
    "print(\"  - location_region (e.g., Northeast, West)\")\n",
    "print(\"  - is_usa (1 for USA, 0 for International)\")\n",
    "\n",
    "print(\"\\nTop 10 states by job count:\")\n",
    "print(state_counts.head(10))\n",
    "\n",
    "# Check if is_usa column exists, if not derive it from location_region\n",
    "if 'is_usa' in df_enriched.columns:\n",
    "    print(f\"\\n✓ USA jobs: {df_enriched['is_usa'].sum()}\")\n",
    "    print(f\"✓ International jobs: {(df_enriched['is_usa'] == 0).sum()}\")\n",
    "elif 'location_region' in df_enriched.columns:\n",
    "    usa_count = (df_enriched['location_region'] == 'USA').sum()\n",
    "    intl_count = (df_enriched['location_region'] == 'International').sum()\n",
    "    print(f\"\\n✓ USA jobs: {usa_count}\")\n",
    "    print(f\"✓ International jobs: {intl_count}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Location region information not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477fb708",
   "metadata": {},
   "source": [
    "### 5.6 Date Enrichment\n",
    "\n",
    "Parse posted dates and create temporal features including job aging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4048b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:07:01,420 - utils.enrichers - INFO - Extracted date features\n",
      "2025-12-19 13:07:01,426 - utils.enrichers - INFO - Created aging feature categories\n",
      "2025-12-19 13:07:01,438 - utils.enrichers - INFO - Created monthly date clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENRICHMENT 6: DATE FEATURES\n",
      "============================================================\n",
      "\n",
      "✓ Reference date: 2025-12-09\n",
      "\n",
      "✓ Created temporal features:\n",
      "  - posted_year\n",
      "  - posted_month\n",
      "  - posted_quarter\n",
      "  - posted_day_of_week\n",
      "  - days_since_posted\n",
      "  - job_age_category (Very Recent, Recent, Moderate, Old, Very Old)\n",
      "\n",
      "Job age distribution:\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENRICHMENT 6: DATE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enrich date data (using reference date: 2025-12-09)\n",
    "reference_date = datetime(2025, 12, 9)\n",
    "date_enricher = DateEnricher(df_enriched, reference_date=reference_date)\n",
    "df_enriched = date_enricher.enrich()\n",
    "\n",
    "print(f\"\\n✓ Reference date: {reference_date.strftime('%Y-%m-%d')}\")\n",
    "print(\"\\n✓ Created temporal features:\")\n",
    "print(\"  - posted_year\")\n",
    "print(\"  - posted_month\")\n",
    "print(\"  - posted_quarter\")\n",
    "print(\"  - posted_day_of_week\")\n",
    "print(\"  - days_since_posted\")\n",
    "print(\"  - job_age_category (Very Recent, Recent, Moderate, Old, Very Old)\")\n",
    "\n",
    "print(\"\\nJob age distribution:\")\n",
    "if 'job_age_category' in df_enriched.columns:\n",
    "    print(df_enriched['job_age_category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370911f",
   "metadata": {},
   "source": [
    "### 5.7 Additional Features\n",
    "\n",
    "Create derived features for employment type and company characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4cf75ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:07:11,600 - utils.enrichers - INFO - Created additional derived features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENRICHMENT 7: ADDITIONAL FEATURES\n",
      "============================================================\n",
      "\n",
      "✓ Created employment type flags:\n",
      "  - is_remote: 452 jobs\n",
      "  - is_contract: 465 jobs\n",
      "\n",
      "✓ Created company features:\n",
      "  - company_size_category (if available)\n",
      "  - industry_category (if available)\n",
      "\n",
      "Final enriched dataset shape: (2000, 74)\n",
      "Total features created: 62 new columns\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENRICHMENT 7: ADDITIONAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enrich additional features\n",
    "additional_enricher = AdditionalFeaturesEnricher(df_enriched)\n",
    "df_enriched = additional_enricher.enrich()\n",
    "\n",
    "print(\"\\n✓ Created employment type flags:\")\n",
    "if 'is_remote' in df_enriched.columns:\n",
    "    print(f\"  - is_remote: {df_enriched['is_remote'].sum()} jobs\")\n",
    "if 'is_full_time' in df_enriched.columns:\n",
    "    print(f\"  - is_full_time: {df_enriched['is_full_time'].sum()} jobs\")\n",
    "if 'is_contract' in df_enriched.columns:\n",
    "    print(f\"  - is_contract: {df_enriched['is_contract'].sum()} jobs\")\n",
    "\n",
    "print(\"\\n✓ Created company features:\")\n",
    "print(\"  - company_size_category (if available)\")\n",
    "print(\"  - industry_category (if available)\")\n",
    "\n",
    "print(f\"\\nFinal enriched dataset shape: {df_enriched.shape}\")\n",
    "print(f\"Total features created: {df_enriched.shape[1] - df_cleaned.shape[1]} new columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95deac7",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data\n",
    "\n",
    "Save both cleaned and enriched datasets for downstream analysis.\n",
    "\n",
    "### 6.1 Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c54bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:07:21,501 - utils.file_handler - INFO - Created directory: data\\cleaned\n",
      "2025-12-19 13:07:21,551 - utils.file_handler - INFO - Saved CSV: data\\cleaned\\ai_job_market_cleaned.csv with shape (2000, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned data saved to: data/cleaned/ai_job_market_cleaned.csv\n",
      "  Shape: (2000, 12)\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "cleaned_path = config.get_path('paths.cleaned_data_file')\n",
    "file_handler.save_csv(df_cleaned, cleaned_path)\n",
    "print(f\"✓ Cleaned data saved to: {cleaned_path}\")\n",
    "print(f\"  Shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea985a44",
   "metadata": {},
   "source": [
    "### 6.2 Save Enriched Data by Category\n",
    "\n",
    "Organize enriched data into separate files by category for efficient access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2fa04ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:07:27,644 - utils.file_handler - INFO - Created directory: data\\enriched\n",
      "2025-12-19 13:07:27,711 - utils.file_handler - INFO - Saved CSV: data\\enriched\\salary_enriched.csv with shape (2000, 9)\n",
      "2025-12-19 13:07:27,753 - utils.file_handler - INFO - Saved CSV: data\\enriched\\skills_enriched.csv with shape (2000, 29)\n",
      "2025-12-19 13:07:27,781 - utils.file_handler - INFO - Saved CSV: data\\enriched\\tools_enriched.csv with shape (2000, 14)\n",
      "2025-12-19 13:07:27,801 - utils.file_handler - INFO - Saved CSV: data\\enriched\\experience_enriched.csv with shape (2000, 6)\n",
      "2025-12-19 13:07:27,822 - utils.file_handler - INFO - Saved CSV: data\\enriched\\location_enriched.csv with shape (2000, 7)\n",
      "2025-12-19 13:07:27,846 - utils.file_handler - INFO - Saved CSV: data\\enriched\\date_enriched.csv with shape (2000, 10)\n",
      "2025-12-19 13:07:27,866 - utils.file_handler - INFO - Saved CSV: data\\enriched\\employment_enriched.csv with shape (2000, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING ENRICHED DATA BY CATEGORY\n",
      "============================================================\n",
      "✓ salary_enriched.csv (9 columns)\n",
      "✓ skills_enriched.csv (29 columns)\n",
      "✓ tools_enriched.csv (14 columns)\n",
      "✓ experience_enriched.csv (6 columns)\n",
      "✓ location_enriched.csv (7 columns)\n",
      "✓ date_enriched.csv (10 columns)\n",
      "✓ employment_enriched.csv (7 columns)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:07:27,887 - utils.file_handler - INFO - Saved CSV: data\\enriched\\company_enriched.csv with shape (2000, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ company_enriched.csv (7 columns)\n",
      "\n",
      "✓ All enriched data saved to: data/enriched/\n"
     ]
    }
   ],
   "source": [
    "from utils.constant import COMMON_COLUMNS\n",
    "\n",
    "# Define enriched data directory\n",
    "enriched_dir = 'data/enriched'\n",
    "file_handler.ensure_directory(enriched_dir)\n",
    "\n",
    "# Define common columns for all categories\n",
    "common_cols = ['job_id', 'job_title', 'company_name', 'location']\n",
    "\n",
    "# Category configurations\n",
    "category_configs = {\n",
    "    'salary': common_cols + [\n",
    "        'salary_range_usd', 'salary_min', 'salary_max', 'salary_avg', 'salary_cluster'\n",
    "    ],\n",
    "    'skills': common_cols + ['skills_required', 'skills_count',\n",
    "                              'has_programming_lang', 'has_cloud_platform', 'has_ml_framework'] + \n",
    "              [col for col in df_enriched.columns if col.startswith('skill_')],\n",
    "    'tools': common_cols + ['tools_preferred', 'tools_count'] +\n",
    "             [col for col in df_enriched.columns if col.startswith('tool_')],\n",
    "    'experience': common_cols + ['experience_level', 'experience_level_ordinal'],\n",
    "    'location': common_cols + ['location_city', 'location_state', 'location_region', 'is_usa'],\n",
    "    'date': common_cols + ['posted_date', 'posted_year', 'posted_month', 'posted_quarter',\n",
    "                           'posted_day_of_week', 'days_since_posted', 'job_age_category'],\n",
    "    'employment': common_cols + ['employment_type', 'is_remote', 'is_full_time', 'is_contract'],\n",
    "    'company': common_cols + ['company_name', 'company_size', 'industry']\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING ENRICHED DATA BY CATEGORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save each category\n",
    "for category, columns in category_configs.items():\n",
    "    # Filter columns that actually exist in the dataframe\n",
    "    available_cols = [col for col in columns if col in df_enriched.columns]\n",
    "    \n",
    "    if available_cols:\n",
    "        df_category = df_enriched[available_cols]\n",
    "        filepath = f\"{enriched_dir}/{category}_enriched.csv\"\n",
    "        file_handler.save_csv(df_category, filepath)\n",
    "        print(f\"✓ {category}_enriched.csv ({len(available_cols)} columns)\")\n",
    "\n",
    "print(f\"\\n✓ All enriched data saved to: {enriched_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd584bc8",
   "metadata": {},
   "source": [
    "### 6.3 Save Data Dictionaries\n",
    "\n",
    "Save frequency counts for skills, tools, and locations for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c711f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 13:07:44,478 - utils.file_handler - INFO - Created directory: data\\dictionary\n",
      "2025-12-19 13:07:44,486 - utils.file_handler - INFO - Saved CSV: data\\dictionary\\skill_frequency.csv with shape (22, 2)\n",
      "2025-12-19 13:07:44,494 - utils.file_handler - INFO - Saved CSV: data\\dictionary\\tool_frequency.csv with shape (8, 2)\n",
      "2025-12-19 13:07:44,520 - utils.file_handler - INFO - Saved CSV: data\\dictionary\\location_frequency.csv with shape (195, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING DATA DICTIONARIES\n",
      "============================================================\n",
      "✓ skill_frequency.csv (22 skills)\n",
      "✓ tool_frequency.csv (8 tools)\n",
      "✓ location_frequency.csv (195 locations)\n",
      "\n",
      "✓ Data dictionaries saved to: data/dictionary/\n"
     ]
    }
   ],
   "source": [
    "# Save data dictionaries\n",
    "dictionary_dir = 'data/dictionary'\n",
    "file_handler.ensure_directory(dictionary_dir)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVING DATA DICTIONARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save skill frequency\n",
    "if 'skill_counts' in locals():\n",
    "    skill_freq_df = pd.DataFrame({\n",
    "        'skill': skill_counts.index,\n",
    "        'frequency': skill_counts.values\n",
    "    })\n",
    "    file_handler.save_csv(skill_freq_df, f\"{dictionary_dir}/skill_frequency.csv\")\n",
    "    print(f\"✓ skill_frequency.csv ({len(skill_freq_df)} skills)\")\n",
    "\n",
    "# Save tool frequency\n",
    "if 'tool_counts' in locals():\n",
    "    tool_freq_df = pd.DataFrame({\n",
    "        'tool': tool_counts.index,\n",
    "        'frequency': tool_counts.values\n",
    "    })\n",
    "    file_handler.save_csv(tool_freq_df, f\"{dictionary_dir}/tool_frequency.csv\")\n",
    "    print(f\"✓ tool_frequency.csv ({len(tool_freq_df)} tools)\")\n",
    "\n",
    "# Save location frequency\n",
    "if 'state_counts' in locals():\n",
    "    location_freq_df = pd.DataFrame({\n",
    "        'location': state_counts.index,\n",
    "        'frequency': state_counts.values\n",
    "    })\n",
    "    file_handler.save_csv(location_freq_df, f\"{dictionary_dir}/location_frequency.csv\")\n",
    "    print(f\"✓ location_frequency.csv ({len(location_freq_df)} locations)\")\n",
    "\n",
    "print(f\"\\n✓ Data dictionaries saved to: {dictionary_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ba1f7",
   "metadata": {},
   "source": [
    "## 7. Final Summary\n",
    "\n",
    "Summary of the complete cleaning and enrichment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34908362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA CLEANING & ENRICHMENT SUMMARY\n",
      "============================================================\n",
      "\n",
      "📊 DATASET TRANSFORMATION:\n",
      "   Raw Data:       2,000 rows × 12 columns\n",
      "   Cleaned Data:   2,000 rows × 12 columns\n",
      "   Enriched Data:  2,000 rows × 74 columns\n",
      "   New Features:   62 columns added\n",
      "\n",
      "🧹 CLEANING OPERATIONS:\n",
      "   Duplicates removed:     0\n",
      "   Data quality improved:  0.0% → 100%\n",
      "\n",
      "✨ ENRICHMENT CATEGORIES:\n",
      "   Salary      :   4 features\n",
      "   Skills      :  20 features\n",
      "   Tools       :   8 features\n",
      "   Experience  :   1 features\n",
      "   Location    :   4 features\n",
      "   Date        :   4 features\n",
      "   Additional  :   3 features\n",
      "\n",
      "💾 OUTPUT FILES:\n",
      "   Cleaned: data/cleaned/ai_job_market_cleaned.csv\n",
      "   Enriched: data/enriched/ (8 category files)\n",
      "   Dictionary: data/dictionary/ (3 frequency files)\n",
      "\n",
      "✅ DATA READY FOR ANALYSIS!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING & ENRICHMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📊 DATASET TRANSFORMATION:\")\n",
    "print(f\"   Raw Data:       {df_raw.shape[0]:,} rows × {df_raw.shape[1]} columns\")\n",
    "print(f\"   Cleaned Data:   {df_cleaned.shape[0]:,} rows × {df_cleaned.shape[1]} columns\")\n",
    "print(f\"   Enriched Data:  {df_enriched.shape[0]:,} rows × {df_enriched.shape[1]} columns\")\n",
    "print(f\"   New Features:   {df_enriched.shape[1] - df_cleaned.shape[1]} columns added\")\n",
    "\n",
    "print(\"\\n🧹 CLEANING OPERATIONS:\")\n",
    "print(f\"   Duplicates removed:     {cleaning_report['total_rows_removed']}\")\n",
    "# Get quality score before cleaning from the original validator\n",
    "# Calculate quality score from raw data validator's summary\n",
    "raw_summary = validator.get_summary()\n",
    "raw_quality_score = raw_summary.get('data_quality_score', 0.0)\n",
    "print(f\"   Data quality improved:  {raw_quality_score:.1f}% → 100%\")\n",
    "\n",
    "print(\"\\n✨ ENRICHMENT CATEGORIES:\")\n",
    "enrichment_summary = {\n",
    "    'Salary': ['salary_min', 'salary_max', 'salary_avg', 'salary_cluster'],\n",
    "    'Skills': [col for col in df_enriched.columns if col.startswith('skill_')],\n",
    "    'Tools': [col for col in df_enriched.columns if col.startswith('tool_')],\n",
    "    'Experience': ['experience_level_ordinal'],\n",
    "    'Location': ['location_city', 'location_state', 'location_region', 'location_cluster'],\n",
    "    'Date': ['posted_year', 'posted_month', 'posted_quarter', 'aging_feature'],\n",
    "    'Additional': ['is_remote', 'is_fulltime', 'is_contract']\n",
    "}\n",
    "\n",
    "for category, features in enrichment_summary.items():\n",
    "    available_features = [f for f in features if f in df_enriched.columns]\n",
    "    print(f\"   {category:12s}: {len(available_features):3d} features\")\n",
    "\n",
    "print(\"\\n💾 OUTPUT FILES:\")\n",
    "print(\"   Cleaned: data/cleaned/ai_job_market_cleaned.csv\")\n",
    "print(\"   Enriched: data/enriched/ (8 category files)\")\n",
    "print(\"   Dictionary: data/dictionary/ (3 frequency files)\")\n",
    "\n",
    "print(\"\\n✅ DATA READY FOR ANALYSIS!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e0553",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Reusable Functions for Other Notebooks\n",
    "\n",
    "These functions can be imported and used in other analysis notebooks.\n",
    "\n",
    "### Usage Example:\n",
    "```python\n",
    "# In another notebook:\n",
    "from cleaning import load_cleaned_data, load_enriched_data\n",
    "\n",
    "# Load cleaned data\n",
    "df = load_cleaned_data()\n",
    "\n",
    "# Load specific enriched category\n",
    "df_salary = load_enriched_data('salary')\n",
    "df_skills = load_enriched_data('skills')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0050de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reusable functions defined:\n",
      "  - load_cleaned_data()\n",
      "  - load_enriched_data(category=None)\n",
      "  - get_skill_frequency()\n",
      "  - get_tool_frequency()\n",
      "  - get_location_frequency()\n"
     ]
    }
   ],
   "source": [
    "def load_cleaned_data() -> pd.DataFrame:\n",
    "    config = get_config_loader()\n",
    "    file_handler = FileHandler()\n",
    "    cleaned_path = config.get_path('paths.cleaned_data_file')\n",
    "    return file_handler.load_csv(cleaned_path)\n",
    "\n",
    "\n",
    "def load_enriched_data(category: str = None) -> pd.DataFrame:\n",
    "\n",
    "    file_handler = FileHandler()\n",
    "    enriched_dir = 'data/enriched'\n",
    "    \n",
    "    if category:\n",
    "        filepath = f\"{enriched_dir}/{category}_enriched.csv\"\n",
    "        return file_handler.load_csv(filepath)\n",
    "    else:\n",
    "        # Load all categories and merge\n",
    "        categories = ['salary', 'skills', 'tools', 'experience', \n",
    "                     'location', 'date', 'employment', 'company']\n",
    "        dfs = []\n",
    "        for cat in categories:\n",
    "            filepath = f\"{enriched_dir}/{cat}_enriched.csv\"\n",
    "            try:\n",
    "                df_cat = file_handler.load_csv(filepath)\n",
    "                dfs.append(df_cat)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Merge on common columns\n",
    "        if dfs:\n",
    "            df_merged = dfs[0]\n",
    "            for df_cat in dfs[1:]:\n",
    "                common_cols = ['job_id', 'job_title', 'company_name', 'location']\n",
    "                merge_cols = [col for col in common_cols if col in df_merged.columns and col in df_cat.columns]\n",
    "                if merge_cols:\n",
    "                    df_merged = df_merged.merge(df_cat, on=merge_cols, how='outer')\n",
    "            return df_merged\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_skill_frequency() -> pd.DataFrame:\n",
    "    file_handler = FileHandler()\n",
    "    return file_handler.load_csv('data/dictionary/skill_frequency.csv')\n",
    "\n",
    "\n",
    "def get_tool_frequency() -> pd.DataFrame:\n",
    "    file_handler = FileHandler()\n",
    "    return file_handler.load_csv('data/dictionary/tool_frequency.csv')\n",
    "\n",
    "\n",
    "def get_location_frequency() -> pd.DataFrame:\n",
    "    file_handler = FileHandler()\n",
    "    return file_handler.load_csv('data/dictionary/location_frequency.csv')\n",
    "\n",
    "\n",
    "print(\"✓ Reusable functions defined:\")\n",
    "print(\"  - load_cleaned_data()\")\n",
    "print(\"  - load_enriched_data(category=None)\")\n",
    "print(\"  - get_skill_frequency()\")\n",
    "print(\"  - get_tool_frequency()\")\n",
    "print(\"  - get_location_frequency()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda64d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Technical Details\n",
    "\n",
    "### Data Cleaning Strategy\n",
    "- **Duplicates**: Removed using pandas `drop_duplicates()` with `keep='first'`\n",
    "- **Missing Values**: Dropped rows with missing critical fields\n",
    "- **Validation**: Post-cleaning validation ensures 100% data quality\n",
    "\n",
    "### Enrichment Architecture\n",
    "Each enricher class follows the **Single Responsibility Principle**:\n",
    "- `SalaryEnricher`: Salary parsing and clustering\n",
    "- `SkillsEnricher`: Skill extraction and binary encoding\n",
    "- `ToolsEnricher`: Tool extraction and binary encoding\n",
    "- `ExperienceEnricher`: Ordinal encoding of experience levels\n",
    "- `LocationEnricher`: Geographic parsing and clustering\n",
    "- `DateEnricher`: Temporal feature extraction\n",
    "- `AdditionalFeaturesEnricher`: Derived features\n",
    "\n",
    "### File Organization\n",
    "```\n",
    "data/\n",
    "├── raw/                         # Original dataset\n",
    "├── cleaned/                     # Cleaned dataset\n",
    "├── enriched/                    # Enriched by category\n",
    "│   ├── salary_enriched.csv\n",
    "│   ├── skills_enriched.csv\n",
    "│   ├── tools_enriched.csv\n",
    "│   ├── experience_enriched.csv\n",
    "│   ├── location_enriched.csv\n",
    "│   ├── date_enriched.csv\n",
    "│   ├── employment_enriched.csv\n",
    "│   └── company_enriched.csv\n",
    "└── dictionary/                  # Reference data\n",
    "    ├── skill_frequency.csv\n",
    "    ├── tool_frequency.csv\n",
    "    └── location_frequency.csv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook** \n",
    "\n",
    "For questions or issues, refer to the project documentation in `README.md` and `ARCHITECTURE.md`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-job-market",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
